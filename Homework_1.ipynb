{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.0.0. \n",
    "\n",
    "**Define big data. Provide an example of a big data problem in your domain of expertise.** \n",
    "\n",
    "Big Data is a term used to describe a massive volume of diverse data, both structured and unstructured, that is so large and \n",
    "fast-moving that it’s difficult or impossible to process using traditional databases and software technology. In most enterprise \n",
    "scenarios, the data is too enormous, streaming by too quickly at unpredictable and variable speeds, and exceeds current \n",
    "processing capacity.\n",
    "\n",
    "**What is a race condition in the context of parallel computation? Give an example.**\n",
    "\n",
    "\n",
    "**What is MapReduce?**\n",
    "\n",
    "MapReduce is a processing technique and a program model for distributed computing based on java. The MapReduce algorithm \n",
    "contains two important tasks, namely Map and Reduce. Map takes a set of data and converts it into another set of data, where \n",
    "individual elements are broken down into tuples (key/value pairs). Secondly, reduce task, which takes the output from a map \n",
    "as an input and combines those data tuples into a smaller set of tuples. As the sequence of the name MapReduce implies, the \n",
    "reduce task is always performed after the map job.\n",
    "\n",
    "**How does it differ from Hadoop?**\n",
    "\n",
    "First off, Map Reduce is an algorithm, Hadoop is an Ecosystem. Both are indepent of underlying hardware or programming \n",
    "language or technology stack. Now, Hadoop leverages this algorithm to split up tasks accross multiple processing nodes and \n",
    "then recombining them. So, you can say the Apache hadoop ecosystem leverages the Map reduce algorithm over a Java platform \n",
    "to do parallel, distibuted multiprocessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## HW1.0.1: \n",
    "\n",
    "Here is an example of functional programming in basic python in terms of mappers and reducers (by way of example):\n",
    "\n",
    "**#EXAMPLE Mapper functions in Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fahrenheit(T):\n",
    "    return ((float(9)/5)*T + 32)\n",
    "\n",
    "def celsius(T):\n",
    "    return (float(5)/9)*(T-32)\n",
    "\n",
    "temperatures = (36.5, 37, 37.5, 38, 39)\n",
    "F = map(fahrenheit, temperatures)\n",
    "#returns  97.7  98.6  99.5 100.4 102.2\n",
    "C = map(celsius, F)\n",
    "\n",
    "#EXAMPLE Reducer function in Python\n",
    "import functools\n",
    "functools.reduce(lambda x,y: x+y, [47,11,42,13])\n",
    "#returns 113\n",
    "\n",
    "import functools as reduce\n",
    "print \"Average temp is %fF\" % (reduce(lambda x,y: x+y, F)/len(F) )\n",
    "#returns Average temp is 99.68F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "**Which programming paradigm is Hadoop based on? Explain and give a simple example of functional programming in raw python code \n",
    "and show the code running. E.g., in raw python find the average length of a string in and of strings using a python \"map-reduce\" \n",
    "(functional programming) job (similar in style to the above). Alternatively, you can do this in python Hadoop Streaming.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strings = [\"str1\", \"string2\", \"w261\", \"MAchine learning at SCALE\"]\n",
    ".......\n",
    "\n",
    "import functools as reduce\n",
    "temperatures = (36.5, 37, 37.5, 38, 39)\n",
    "F = map(fahrenheit, temperatures)\n",
    "print \"Average temp is %fF\" % (reduce(lambda x,y: x+y, F)/len(F) )\n",
    "#returns Average temp is 99.68F\n",
    "\n",
    "map(sqr, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MapReduce example - avg length of strings\n",
    "import functools\n",
    "\n",
    "def length_string(S):\n",
    "    return(len(S))\n",
    "\n",
    "strings = [\"str1\", \"string2\", \"w261\", \"MAchine learning at SCALE\"]\n",
    "\n",
    "A = map(length_string, strings)\n",
    "\n",
    "print('Average (Mean) Length for the input strings is %i' % (functools.reduce(lambda x, y: x + y, A) / len(A)))\n",
    "\n",
    "# print('Average (Mean) Length for the input strings is {:f}'.format(functools.reduce(lambda x, y: x + y, A) / len(A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.1 Cross fold validation (*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.2: WORDCOUNT\n",
    "\n",
    "Using the Enron dataset and Hadoop MapReduce streaming (or MRJob), write the mapper/reducer job that will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). Examine the word “assistance” and report its word count results.\n",
    "CROSSCHECK: >grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l\n",
    "8\n",
    "   #NOTE  \"assistance\" occurs on 8 lines but how many times does the token occur? 10 times! This is the number we are looking for!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "%%writefile wordcount.py   \n",
    "# Note that this code works even when reducer is not set to 1\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import re, string\n",
    "\n",
    "class MRJobWordCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        # Creates regular expression to modify punctuation characters so that they will not mess up regex\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        # Splits the lines by the first 2 tabs. Keeps everything after the second tab. \n",
    "        token = line.strip().split('\\t', 2)[-1]\n",
    "        # Uses the regex above to replace puctuation with a ' ' and puts all words in lower case\n",
    "        token = regex.sub(' ', token.lower())\n",
    "        # Converts all white space that is not a ' ' into a ' ', including duplicate spaces\n",
    "        token = re.sub( '\\s+', ' ', token )\n",
    "        \n",
    "        # Creates a list of the words\n",
    "        words = token.split()\n",
    "\n",
    "        # for loop that results in only words greater than length 1 being yielded\n",
    "        for word in words:\n",
    "            if len(word) > 1:\n",
    "                yield (word, 1)\n",
    "   \n",
    "    # Creats sum by word for each mapper - like a per-mapper reducer\n",
    "    def combiner(self, word, counts):\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "    # Creates an empty dictionary called results, then sums \n",
    "    def reducer(self, word, counts):\n",
    "        yield (word, sum(counts))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MRJobWordCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python wordcount.py enronemail_1h.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check the number of times assistance appears \n",
    "!grep -o assistance enronemail_1h.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output word counts to txt for later use\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from numpy import random\n",
    "from wordcount import MRJobWordCount \n",
    "     \n",
    "count_data = 'enronemail_1h.txt'\n",
    "\n",
    "mr_job = MRJobWordCount(args=[count_data])\n",
    "model_stats = {}\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access to the output reducer/reducer_final of \n",
    "    # the last step in MRJobWordCount\n",
    "    for line in runner.stream_output():\n",
    "        key, value =  mr_job.parse_output_line(line)\n",
    "        print key, value\n",
    "        model_stats[key] = value            \n",
    "    # Store model locally\n",
    "    with open('model1.txt', 'w') as f:\n",
    "        for k in model_stats.keys():\n",
    "            f.writelines( k + \"\\t\"+ str(model_stats[k]) +\"\\n\")\n",
    "print model_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_stats = {}\n",
    "records = [s.split('\\n')[0].split('\\t') for s in open(\"model1.txt\").readlines()]\n",
    "for word, count in records:\n",
    "    model_stats[word] =  map(int, count.split(\",\"))\n",
    "model_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## HW1.2.1\n",
    "\n",
    "Using Hadoop MapReduce (or MRJob) and your wordcount job (from HW1.2) determine the top-10 occurring tokens \n",
    "(most frequent tokens) using a single reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Using model_stats created in MRJob above to get 10 most common words\n",
    "Counter(model_stats).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## HW1.3: Multinomial NAIVE BAYES with NO Smoothing using a single reducer\n",
    "\n",
    "Using the Enron data from HW1 and Hadoop MapReduce (or MRJob), write a mapper/reducer job(s) that will both learn Naive Bayes \n",
    "classifier and classify the Enron email messages using the learnt Naive Bayes classifier. Use all white-space delimitted tokens \n",
    "as independent input variables (assume spaces, fullstops, commas as delimiters). Note: for multinomial Naive Bayes, \n",
    "the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "\n",
    "    the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM\n",
    "\n",
    "    E.g., “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in \n",
    "all documents labeled as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial \n",
    "estimation of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW. Multiplying lots of \n",
    "probabilities, which are between 0 and 1, can result in floating-point underflow. Since log(xy) = log(x) + log(y), it is better \n",
    "to perform all computations by summing logs of probabilities rather than multiplying probabilities. Please pay attention to \n",
    "probabilites that are zero! They will need special attention. Count up how many times you need to process a zero probabilty \n",
    "for each class and report.\n",
    "\n",
    "    Report the performance of your learnt classifier in terms of misclassifcation error rate of your multinomial Naive Bayes \n",
    "    Classifier. Plot a histogram of the posterior probabilities (i.e., Pr(Class|Doc)) for each class over the training set. \n",
    "    Summarize what you see.\n",
    "\n",
    "    Error Rate = misclassification rate with respect to a provided set (say training set in this case). It is more formally \n",
    "    defined here:\n",
    "\n",
    "Let DF represent the evalution set in the following: Err(Model, DF) = |{(X, c(X)) ∈ DF : c(X) != Model(x)}| / |DF|\n",
    "\n",
    "Where || denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by \n",
    "the Model “Model”\n",
    "\n",
    "NOTE: please assume one reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Frist splitting data into 70% for training, 15% for valdiation, and 15% for testing \n",
    "# Works even if number of lines != 100\n",
    "import random\n",
    "\n",
    "with open(\"enronemail_1h.txt\", \"r\") as f:\n",
    "    data = f.read().split('\\n')\n",
    "\n",
    "# Randomly sort data to avoid any bias from the order in original data\n",
    "random.shuffle(data)\n",
    "\n",
    "num_lines = len(data)\n",
    "num_lines_70pct = int(.7*num_lines)\n",
    "num_lines_85pct = int(.85*num_lines)\n",
    "\n",
    "\n",
    "train_data = data[:num_lines_70pct]\n",
    "validation_data = data[num_lines_70pct:num_lines_85pct]\n",
    "testing_data = data[num_lines_85pct:]\n",
    "\n",
    "with open(\"enron_training_data.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(train_data))\n",
    "with open(\"enron_validation_data.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(validation_data))\n",
    "with open(\"enron_testing_data.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(testing_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile naive_bayes_enron.py\n",
    "\n",
    "# NOTE: I could set this to use one reducer as was asked, but there is no need to with the code below. \n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    " \n",
    "from collections import defaultdict\n",
    " \n",
    "from mrjob.job import MRJob\n",
    "from mrjob.job import MRStep\n",
    "\n",
    "import re, string\n",
    "\n",
    "# Stores the number of emails by classifier\n",
    "email_counts = dict()\n",
    " \n",
    "# Stores the number of words by classifier\n",
    "word_counts = dict()\n",
    "\n",
    "class NaiveBayesEnron(MRJob):\n",
    "    \"\"\"\n",
    "    A MRJob class for a Naive Bayes probability domain calculation of whether an email\n",
    "    is SPAM or HAM (not-spam).\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(NaiveBayesEnron, self).__init__(*args, **kwargs)\n",
    "     \n",
    "    def configure_options(self):\n",
    "        \"\"\"\n",
    "        Adding this allows switching between smoothing methods w/o having to copy all code \n",
    "        for each method and for changing the min length of words included in the model\n",
    "        \"\"\"\n",
    "        super(NaiveBayesEnron, self).configure_options()\n",
    "        self.add_passthrough_option(\n",
    "            '--smoothing', default='none', choices=['none', 'laplace', 'jm'], \n",
    "            help='Specifies the type of smoothing to use'\n",
    "        )\n",
    "    \n",
    "        self.add_passthrough_option(\n",
    "            '--min-word-frequency', default=1, dest='min_word_frequency', type='int',\n",
    "            help='Specifies the min length of a word to be included in model'\n",
    "        )\n",
    "        \n",
    "        self.add_passthrough_option(\n",
    "            '--lambda', default=0.3, dest='jm_lambda', type='float',\n",
    "            help='Specifies the desired lambda value for JM smoothing'\n",
    "        )\n",
    "        \n",
    "    def steps(self):\n",
    "        out = [\n",
    "            MRStep(\n",
    "                mapper = self.mapper_one,\n",
    "                combiner = self.combiner_one,\n",
    "                reducer = self.reducer_one\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        if self.options.smoothing == 'laplace': \n",
    "            out.append(MRStep(\n",
    "                reducer = self.reducer_laplace\n",
    "            ))\n",
    "        \n",
    "        elif self.options.smoothing == 'jm':\n",
    "            out.append(MRStep(\n",
    "                reducer = self.reducer_jm\n",
    "            ))\n",
    "            \n",
    "        else:\n",
    "            out.append(MRStep(\n",
    "                reducer = self.reducer_unsmoothed\n",
    "            ))\n",
    "        \n",
    "        return out\n",
    " \n",
    "    def mapper_one(self, _, email):\n",
    "        \"\"\"\n",
    "        Read each email and map it to a collection of words and\n",
    "        counts\n",
    " \n",
    "        :param _:\n",
    "            There is no key here because we're loading raw text\n",
    "        :param email:\n",
    "            An email to be processed\n",
    "        :return:\n",
    "            Yields a generator that returns (key, value) tuples for each unique\n",
    "            word present in the email.\n",
    "                * key: a tuple containing the word and the classifier for\n",
    "                    the email\n",
    "                * value: the number of times that word appeared in the email\n",
    "        \"\"\"\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        _, classifier, token = email.strip().split('\\t', 2)\n",
    "        token = regex.sub(' ', token.lower())\n",
    "        token = re.sub( '\\s+', ' ', token )\n",
    "        \n",
    "        words = token.split()\n",
    " \n",
    "        # Counter for the number of emails processed by classifier\n",
    "        yield (('EMAILS', classifier), 1)\n",
    " \n",
    "        for word in set(words):                \n",
    "            count = words.count(word)\n",
    " \n",
    "            yield ((word, classifier), count)\n",
    " \n",
    "            # Counter for the number of words processed by classifier\n",
    "            yield (('WORDS', classifier), count)\n",
    " \n",
    " \n",
    "    def combiner_one(self, word_classifier, counts):\n",
    "        \"\"\"\n",
    "        Combine the raw output of each mapper so that the value for each key\n",
    "        is now the number of times each word-classifier appeared in that\n",
    "        mapper (not just in a particular email). This reduces the amount of\n",
    "        data each mapper sends to the reducer.\n",
    " \n",
    "        :param word_classifier:\n",
    "            A tuple with a word and classifier combination\n",
    "        :param counts:\n",
    "            The number of counts that word + classifier combination was found\n",
    "            in each email processed by a particular mapper.\n",
    "        :return:\n",
    "            Yields a generator that returns (key, value) tuples for each unique\n",
    "            word processed by a particular mapper.\n",
    "                * key: a tuple containing the word and the classifier for\n",
    "                    the email\n",
    "                * value: the number of times that word + classifier\n",
    "                    combination appeared in the mapper\n",
    "        \"\"\"\n",
    " \n",
    "        yield (word_classifier, sum(counts))\n",
    " \n",
    "    def reducer_one(self, word_classifier, counts):\n",
    "        \"\"\"\n",
    "        Collects the results of each mapper to get the total counts for each\n",
    "        word + classifier combination within the processed document.\n",
    " \n",
    "        :param word_classifier:\n",
    "            A tuple with a word and classifier combination\n",
    "        :param counts:\n",
    "            The number of counts that word + classifier combination was found\n",
    "            in each email processed by a particular mapper.\n",
    "        :return:\n",
    "            Yields a generator that returns (key, value) tuples for each unique\n",
    "            word processed by a particular mapper.\n",
    "                * key: a tuple containing the word and the classifier for\n",
    "                    the email\n",
    "                * value: the number of times that word + classifier\n",
    "                    combination appeared in the mapper\n",
    "        \"\"\"\n",
    "        # Calculate the count for each key \n",
    "        # e.g., for key (word, classifier) sum the list coming in from the combiner/mapper \n",
    "        # Could look like this: (the, 0):[1,4,1] will turn into (the, 0):6\n",
    "        total_count = sum(counts)\n",
    "        # split up the word_classifier tuple into word, classifier\n",
    "        word, classifier = word_classifier\n",
    " \n",
    "        # store the total number of words for a paticular classifier\n",
    "        # By time the reducer is done, this will add the word count for each classifier to the word_counts dict\n",
    "        # adding the return at the end of this it discards the \"WORDS\" keys \n",
    "        # as they are no longer needed after getting total_count\n",
    "        if word == 'WORDS':\n",
    "            if classifier not in word_counts:\n",
    "                word_counts[classifier] = 0\n",
    "                \n",
    "            word_counts[classifier] += total_count\n",
    "            return\n",
    "        \n",
    "        # store the total number of emails for a paticular classifier\n",
    "        # By time the reducer is done, this will add the number of emails for each classifier to the email_counts dict \n",
    "        # Changed word from 'EMAILS' to 'PRIOR' because how we calculate priors is with the total num emails per classifier\n",
    "        if word == 'EMAILS':\n",
    "            email_counts[classifier] = total_count\n",
    "            word = 'PRIOR'\n",
    " \n",
    "        # This will be reduced again as we may get outputs for the same key more than once (if word appeared in both classifiers)\n",
    "        # for example, (the, {0:3}) and (the, {1:5})\n",
    "        # What the next reducer will receive in this case will look like this: (the, [{0:3}, {1:5}])\n",
    "        if total_count < self.options.min_word_frequency:\n",
    "            # Removes words from the total count if they were below the threshold and doesn't yield them to the\n",
    "            # second stage reducers\n",
    "            if classifier not in word_counts:\n",
    "                word_counts[classifier] = 0\n",
    "            word_counts[classifier] -= total_count\n",
    "        else:\n",
    "            yield (word, {classifier: total_count})\n",
    " \n",
    "    def reducer_unsmoothed(self, word, classified_counts):\n",
    "        \"\"\"\n",
    "        After the first reducer has completed, which populates the global\n",
    "        variables needed to calculate probabilities from word counts, the\n",
    "        results stream is processed to return the smoothed probabilities\n",
    " \n",
    "        :param word:\n",
    "            A unique word that was processed within the document\n",
    "        :param classified_counts:\n",
    "            A collection of dictionaries where the keys in each dictionary\n",
    "            are the classifiers and the values are the number of times that\n",
    "            word appeared within that classifier.\n",
    "        :return:\n",
    "            Prints the final Naive Bayes probabilities for later use\n",
    "        \"\"\"\n",
    " \n",
    "        # dictionary where if the key doesn't exist, just give me 0\n",
    "        combined = defaultdict(lambda: 0)\n",
    "    \n",
    "        # Creatse a single dictionary for a given word, with the key = classifier, and value = count\n",
    "        # Using the example above this will result in {0:3, 1:5}\n",
    "        for entry in classified_counts:\n",
    "            for classifier, count in entry.items():\n",
    "                combined[classifier] += count\n",
    " \n",
    "        # for each classifier calculate the probability that the given word occured\n",
    "        # if the word is 'PRIOR' it calculates the prior probability for that classifier\n",
    "        for classifier in email_counts.keys():\n",
    "            count = combined.get(classifier, 0)\n",
    " \n",
    "            if word == 'PRIOR':\n",
    "                probability = count / sum(email_counts.values())\n",
    "            else:\n",
    "                probability = count / word_counts[classifier]\n",
    " \n",
    "            yield (word, classifier), probability\n",
    "    \n",
    "    def reducer_laplace(self, word, classified_counts):\n",
    "        \"\"\"\n",
    "        This is the laplace smoothed version of the second stage reducer (reducer_unsmoothed)\n",
    "        \"\"\"\n",
    "        \n",
    "        combined = defaultdict(lambda: 0)\n",
    "        \n",
    "        for entry in classified_counts:\n",
    "            for classifier, count in entry.items():\n",
    "                combined[classifier] += count\n",
    " \n",
    "        for classifier in email_counts.keys():\n",
    "            count = combined.get(classifier, 0)\n",
    " \n",
    "            if word == 'PRIOR':\n",
    "                probability = count / sum(email_counts.values())\n",
    "            else:\n",
    "                probability = (count + 1) / (word_counts[classifier] + 2)\n",
    " \n",
    "            yield (word, classifier), probability\n",
    "    \n",
    "    def reducer_jm(self, word, classified_counts):\n",
    "        \"\"\"\n",
    "        This is the Jelinek-Mercer smoothed version of the second stage reducer (reducer_unsmoothed)\n",
    "        \"\"\"\n",
    "        \n",
    "        combined = defaultdict(lambda: 0)\n",
    "        \n",
    "        for entry in classified_counts:\n",
    "            for classifier, count in entry.items():\n",
    "                combined[classifier] += count\n",
    " \n",
    "        for classifier in email_counts.keys():\n",
    "            count = combined.get(classifier, 0)\n",
    "\n",
    "        for classifier in email_counts.keys():\n",
    "            count = combined.get(classifier, 0)\n",
    "            jm_lambda = self.options.jm_lambda\n",
    "        \n",
    "            if word == 'PRIOR':\n",
    "                probability = count / sum(email_counts.values())\n",
    "            else:\n",
    "                total_this_word = sum(combined.values())\n",
    "                total_all_words = sum(word_counts.values())\n",
    "                \n",
    "                probability = (\n",
    "                    (1 - jm_lambda) * (count / word_counts[classifier]) +\n",
    "                    (jm_lambda * total_this_word / total_all_words)\n",
    "                )\n",
    "                \n",
    "            yield (word, classifier), probability \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    NaiveBayesEnron.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build model code\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import naive_bayes_enron as nbe \n",
    "\n",
    "\n",
    "def build_model(training_filename, output_filename, smoothing_type='none', min_word_frequency=1, jm_lambda=0.3):\n",
    "    nbe.word_counts = dict()\n",
    "    nbe.email_counts = dict()\n",
    "    mr_job = nbe.NaiveBayesEnron(\n",
    "        args=[\n",
    "            training_filename,\n",
    "            '--smoothing={}'.format(smoothing_type),\n",
    "            '--min-word-frequency={}'.format(min_word_frequency),\n",
    "            '--lambda={}'.format(jm_lambda)\n",
    "        ]\n",
    "    )\n",
    "    model_stats = dict()\n",
    "    \n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        # stream_output: get access to the output reducer/reducer_final of \n",
    "        # the last step in NaiveBayesEnron\n",
    "        for line in runner.stream_output():\n",
    "            key, value =  mr_job.parse_output_line(line)\n",
    "            word = key[0]\n",
    "            classifier = int(key[1])\n",
    "\n",
    "            if word not in model_stats:\n",
    "                probabilities = ['0', '0']\n",
    "                probabilities[classifier] = str(value)\n",
    "                model_stats[word] = probabilities                        \n",
    "            else:\n",
    "                model_stats[word][classifier] = str(value)\n",
    "\n",
    "        # Store model locally\n",
    "        with open(output_filename, 'w') as f:\n",
    "            for word, probabilities in model_stats.items():\n",
    "                f.writelines(word + \"\\t\" + \"\\t\".join(probabilities) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build unsmoothed model    \n",
    "build_model(    \n",
    "    training_filename='enron_training_data.txt',\n",
    "    smoothing_type='none',\n",
    "    output_filename='enron_model_unsmoothed.txt',\n",
    "    min_word_frequency=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile naive_bayes_enron_classifier.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import os, re, string, math\n",
    "\n",
    "counts = []\n",
    "email_posteriors = dict()\n",
    "\n",
    "class NaiveBayesEnronClassifier(MRJob):\n",
    "\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(NaiveBayesEnronClassifier, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper_init=self.mapper_init, \n",
    "                mapper=self.mapper,\n",
    "                combiner=self.combiner,\n",
    "                reducer=self.reducer  \n",
    "            ),\n",
    "            MRStep(\n",
    "                reducer=self.reducer_two\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def configure_options(self):\n",
    "        super(NaiveBayesEnronClassifier, self).configure_options()\n",
    "        \n",
    "        self.add_file_option('--model')\n",
    "        \n",
    "    def mapper_init(self): \n",
    "        self.model_stats = {}\n",
    "\n",
    "        with open(self.options.model, \"r\") as f:\n",
    "            lines = f.read().split('\\n')\n",
    "        \n",
    "        split_lines = [line.split('\\t') for line in lines]\n",
    "        del lines # Deleting the the line variable to save memory\n",
    "        \n",
    "        for entry in split_lines:\n",
    "            word = entry[0]\n",
    "            probabilities = [float(p) for p in entry[1:]]\n",
    "            self.model_stats[word] = probabilities\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        _, classifier, token = line.strip().split('\\t', 2)\n",
    "        token = regex.sub(' ', token.lower())\n",
    "        token = re.sub( '\\s+', ' ', token )\n",
    "        \n",
    "        # set initial values to the priors obtainined in model_stats\n",
    "        posteriors = [math.log10(prior) for prior in self.model_stats['PRIOR']]\n",
    "        \n",
    "        for word in token.split():\n",
    "            # Get the value for the word, or if it is not in model_stats, return 0 as the default\n",
    "            probabilities = self.model_stats.get(word, [0, 0]) \n",
    "            \n",
    "            # converts all 0s to 1s - note that log10 of 1 = 0\n",
    "            probabilities = [p if p > 0 else 1 for p in probabilities] \n",
    "           \n",
    "            for index, probability in enumerate(probabilities):\n",
    "                if probability == 1:\n",
    "                    yield('zero_count_{}'.format(index), 1)\n",
    "        \n",
    "            posteriors = [x + math.log10(y) for x, y in zip(posteriors, probabilities)]\n",
    "            \n",
    "        for index, posterior in enumerate(posteriors):\n",
    "            yield('post_{}'.format(index), posterior)\n",
    "            \n",
    "        # Determine which class the email belongs to (SPAM vs HAM)\n",
    "        # If p_class_0 = p_class_1, class is unknown\n",
    "        max_posterior = max(posteriors)\n",
    "        max_counts = [1 if p == max_posterior else 0 for p in posteriors]\n",
    "        if sum(max_counts) > 1:\n",
    "            predicted_class = -1\n",
    "        else:\n",
    "            predicted_class = posteriors.index(max_posterior)\n",
    "            \n",
    "        # Determine if our predicted class is correct\n",
    "        if predicted_class == int(classifier):\n",
    "            key = 'correct'\n",
    "        else:\n",
    "            key = 'incorrect'\n",
    "            \n",
    "        yield (key, 1)\n",
    "        yield ('email_count', 1)\n",
    "\n",
    "    def combiner(self, key, values):\n",
    "        \n",
    "        if key.startswith('post_'):\n",
    "            # Keep a list of the posteriors for each class type for histogram\n",
    "            key = key.split('_')[-1]\n",
    "            if key not in email_posteriors:\n",
    "                email_posteriors[key] = []\n",
    "            email_posteriors[key] += list(values)\n",
    "            return\n",
    "        \n",
    "        yield (key, sum(values))\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        \n",
    "        if key.startswith('post_'):\n",
    "            # Keep a list of the posteriors for each class type for histogram\n",
    "            key = key.split('_')[-1]\n",
    "            if key not in email_posteriors:\n",
    "                email_posteriors[key] = []\n",
    "            email_posteriors[key] += list(values)\n",
    "            return\n",
    "        \n",
    "        count = sum(values)\n",
    "        \n",
    "        if key in ['correct', 'incorrect']:\n",
    "            counts.append(count)\n",
    "            \n",
    "        yield (key, count)\n",
    "      \n",
    "    def reducer_two(self, key, values):\n",
    "        values = list(values)\n",
    "        \n",
    "        if key in ['correct', 'incorrect']:\n",
    "            rate = sum(values) / sum(counts)\n",
    "            out_key = 'error_rate' if key == 'incorrect' else 'accuracy'\n",
    "            yield (out_key, rate)\n",
    "        else:\n",
    "            yield (key, sum(values))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    NaiveBayesEnronClassifier.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import naive_bayes_enron_classifier as nbec\n",
    "\n",
    "\n",
    "def run_classifier(trial_name, testing_filename, model_filename):\n",
    "    model_path = os.path.join(\n",
    "        os.path.abspath(os.path.curdir), \n",
    "        model_filename\n",
    "    )\n",
    "    nbec.counts = []\n",
    "    nbec.email_posteriors = dict()\n",
    "    mr_job = nbec.NaiveBayesEnronClassifier(\n",
    "        args=[\n",
    "            testing_filename,\n",
    "            '--model={}'.format(model_path)\n",
    "        ]\n",
    "    )\n",
    "    out = {'trial_name': trial_name, 'error_rate': 0, 'accuracy': 0, 'posteriors': nbec.email_posteriors}\n",
    "    \n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        for line in runner.stream_output():\n",
    "            key, value =  mr_job.parse_output_line(line)\n",
    "            out[key] = value\n",
    "                \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = run_classifier('unsmoothed_test', 'enron_testing_data.txt', 'enron_model_unsmoothed.txt')\n",
    "print('\\n'.join(['* {}: {}'.format(k, v) for k, v in results.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count up how many times you need to process a zero probabilty for each class and report.\n",
    "\n",
    "print('Zero Probability Counts:')\n",
    "print('------------------------')\n",
    "for key, value in results.items():\n",
    "    if key.startswith('zero_count_'):\n",
    "        print('Classifier[{}] = {}'.format(\n",
    "                key.rsplit('_', 1)[-1],\n",
    "                value\n",
    "        ))\n",
    "\n",
    "# Note that I split my data into training/test/validation from the beginning, so my numbers might not match yours\n",
    "# I did this because if you use the full data set for training and testing, it will falsely inflate your accuracy\n",
    "# Providing counts from my 15% testing data\n",
    "\n",
    "for classifier, posteriors in results['posteriors'].items(): \n",
    "    n, bins, patches = plt.hist(posteriors, 20, normed=1)\n",
    "\n",
    "    plt.xlabel('Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('{} Classifier Posterior Probabilities'.format(classifier))\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## HW1.4: Multinomial Naive Bayes with Smoothing\n",
    "\n",
    "**HW1.4.0: Repeat HW1.3 with the following modification: use Laplace plus-one smoothing. Compare the misclassifcation error rates \n",
    "for HW1.3 versus HW1.4 and explain the differences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "build_model(    \n",
    "    training_filename='enron_training_data.txt',\n",
    "    smoothing_type='laplace',\n",
    "    output_filename='enron_model_laplace.txt',\n",
    "    min_word_frequency=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_classifier('laplace_test', 'enron_testing_data.txt', 'enron_model_laplace.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## HW 1.4.1 Jelinek-Mercer (JM) smoothing*\n",
    "\n",
    "With different smoothing methods, p(wk|ci) (i.e., the word class conditionals) will be computed differently. We consider \n",
    "Jelinek-Mercer (JM) smoothing as an alternative to Laplace Let c(w, ci) denote the frequency of word w in category ci, p(w|C) \n",
    "be the maximum likelihood estimation of word w in collection C (relative frequency) and let |C for classi| denote the length \n",
    "of the classi. Then:\n",
    "    \n",
    "1) Jelinek-Mercer (JM) smoothing:\n",
    "\n",
    "λp(w|ci) = (1 − λ) * c(w, ci)/sum_over_wJ_in_V(c(wJ, ci)) + λ p(w|C)\n",
    "\n",
    "Where c(w, ci)/sum_over_wJ_in_V(c(wJ, ci)) essential denotes the relative frequency of word w in class ci, i.e., Pr(w|ci) and \n",
    "one can set λ = 0.3 by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "build_model(    \n",
    "    training_filename='enron_training_data.txt',\n",
    "    smoothing_type='jm',\n",
    "    output_filename='enron_model_jm.txt',\n",
    "    min_word_frequency=1,\n",
    "    jm_lambda=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_classifier('jm_test', 'enron_testing_data.txt', 'enron_model_jm.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## HW1.4.2 Split data in to training, validation and testing data subsets\n",
    "\n",
    "Split the data using MRJob into three subsets in the following proportions (70% for training, 15% for valdiation, \n",
    "and 15% for testing). Train Multinomial Naive Bayes classifiers using Laplace plus-one smoothing and using Jelinek-Mercer (JM)\n",
    "smoothing where you consider different hyperparameter values for λ. Please consider λ in {0.0, 0.1, 0.3, 0.5, 0.7, 1}. Present \n",
    "a table compare the results of the different approaches: each row is the approach taken (e.g., Multinomial Naive Bayes with \n",
    "Laplace+1, or Multinomial Naive Bayes with with JM= 0.3 for λ =0.3) and a column for error rate on the training, validation and \n",
    "test data sets. Present a graph also (in python) consisting of three curves (where the x-axis represents the approach taken and \n",
    "the y-axis represents the error rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "testing_filename = 'enron_testing_data.txt'\n",
    "validation_filename = 'enron_validation_data.txt'\n",
    "model_filename = 'enron_model_trial.txt'\n",
    "trials = []\n",
    "\n",
    "def run_trial(trial_name, smoothing_type, jm_lambda=0.3, min_word_frequency=1):\n",
    "    \n",
    "    build_model(    \n",
    "        training_filename='enron_training_data.txt',\n",
    "        smoothing_type=smoothing_type,\n",
    "        output_filename=model_filename,\n",
    "        min_word_frequency=min_word_frequency,\n",
    "        jm_lambda=jm_lambda\n",
    "    )\n",
    "    \n",
    "    out = {'name': trial_name}\n",
    "    \n",
    "    results = run_classifier(trial_name, testing_filename, model_filename)\n",
    "    out['error_test'] = results['error_rate']\n",
    "    \n",
    "    results = run_classifier(trial_name, validation_filename, model_filename)\n",
    "    out['error_validate'] = results['error_rate']\n",
    "    \n",
    "    return out\n",
    "\n",
    "trials.append(run_trial('unsmoothed', 'none'))\n",
    "trials.append(run_trial('laplace', 'laplace'))\n",
    "trials.append(run_trial('jm = 0.0', 'jm', jm_lambda=0.0))\n",
    "trials.append(run_trial('jm = 0.1', 'jm', jm_lambda=0.1))\n",
    "trials.append(run_trial('jm = 0.3', 'jm', jm_lambda=0.3))\n",
    "trials.append(run_trial('jm = 0.5', 'jm', jm_lambda=0.5))\n",
    "trials.append(run_trial('jm = 0.7', 'jm', jm_lambda=0.7))\n",
    "trials.append(run_trial('jm = 1.0', 'jm', jm_lambda=1.0))\n",
    "\n",
    "df = pd.DataFrame(trials)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "indexes = 10 * df.index\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "r1 = ax.bar(indexes, df.error_test, width=4)\n",
    "r2 = ax.bar(indexes + 4, df.error_validate, width=4, color='y')\n",
    "ax.legend((r1[0], r2[0]), ('Test', 'Validation'))\n",
    "\n",
    "plt.xticks(indexes, df.name, rotation=70)\n",
    "plt.ylabel('Error Rate')\n",
    "plt.xlabel('Smooting Type')\n",
    "plt.title('NB Error Rates By Smoothing Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## HW1.5: Remove words with frequency of less than three (3) in the training set\n",
    "Repeat HW1.4. This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training \n",
    "set. How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset. \n",
    "Report the error and the change in error. HINT: ignore tokens with a frequency of less than three (3). Think of this as a \n",
    "preprocessing step. How many new mapreduce jobs do you need to solve thus homework?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "trials = []\n",
    "\n",
    "trials.append(run_trial('unsmoothed', 'none', min_word_frequency=3))\n",
    "trials.append(run_trial('laplace', 'laplace', min_word_frequency=3))\n",
    "trials.append(run_trial('jm = 0.0', 'jm', jm_lambda=0.0, min_word_frequency=3))\n",
    "trials.append(run_trial('jm = 0.1', 'jm', jm_lambda=0.1, min_word_frequency=3))\n",
    "trials.append(run_trial('jm = 0.3', 'jm', jm_lambda=0.3, min_word_frequency=3))\n",
    "trials.append(run_trial('jm = 0.5', 'jm', jm_lambda=0.5, min_word_frequency=3))\n",
    "trials.append(run_trial('jm = 0.7', 'jm', jm_lambda=0.7, min_word_frequency=3))\n",
    "trials.append(run_trial('jm = 1.0', 'jm', jm_lambda=1.0, min_word_frequency=3))\n",
    "\n",
    "df = pd.DataFrame(trials)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.6 Benchmark your code with the Python SciKit-Learn implementation of the multinomial Naive Bayes algorithm\n",
    "\n",
    "## HW1.6.0: Multinomial Naive Bayes using SciKit-Learn\n",
    "\n",
    "It always a good idea to benchmark your solutions against publicly available libraries such as SciKit-Learn, The Machine \n",
    "Learning toolkit available in Python. In this exercise, we benchmark ourselves against the SciKit-Learn implementation of \n",
    "multinomial Naive Bayes. For more information on this implementation see: \n",
    "http://scikit-learn.org/stable/modules/naive_bayes.html more In this exercise, please complete the following:\n",
    "    \n",
    "-Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in \n",
    "HW1.4.2 and report the misclassification error (please note some data preparation might be needed to get the Multinomial Naive \n",
    "Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "\n",
    "-Prepare a table to present your results, where rows correspond to approach used (SkiKit-Learn versus your Hadoop \n",
    "implementation) and the column presents the misclassification error rates (train, validation, testing) \n",
    "\n",
    "-Explain/justify any differences in terms \n",
    "of training error rates over the dataset in HW1.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus \n",
    "the Multinomial Naive Bayes implementation in SciKit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# read email message, and organize training data\n",
    "with open('enronemail_1h.txt', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    emails = list(reader)\n",
    "train_label = [msg[1] for msg in emails]\n",
    "train_data = [msg[2] + msg[3] if len(msg) == 4 else msg[2] for msg in emails]\n",
    "msg_id = [msg[0].lower() for msg in emails]\n",
    "# print(train_label, train_data, msg_id)\n",
    "\n",
    "# feature vectorization\n",
    "uniVectorizer = CountVectorizer()\n",
    "dtmTrain = uniVectorizer.fit_transform(train_data) \n",
    "# print(uniVectorizer, dtmTrain)\n",
    "\n",
    "# multinomial Naive Bayes Classifier from sklearn\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(dtmTrain, train_label)\n",
    "pred_mnb = mnb.predict(dtmTrain)\n",
    "training_error_mnb = 1.0 * sum(pred_mnb != train_label) / len(train_label)\n",
    "\n",
    "# Bernoulli Naive Bayes Classifier from sklearn\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(dtmTrain, train_label)\n",
    "pred_bnb = bnb.predict(dtmTrain)\n",
    "training_error_bnb = 1.0*sum(pred_bnb != train_label) / len(train_label)\n",
    "\n",
    "print 'SK- multinomial NB training error: %.4f' %training_error_mnb\n",
    "print 'SK- Bernoulli   NB training error: %.4f' %training_error_bnb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
